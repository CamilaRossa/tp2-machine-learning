{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea0f1b-2717-4aa7-a4dd-1a9b2a384d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43232a9a-456f-4242-832e-32ad75fe0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDSCAPE_TYPE = 'buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'\n",
    "\n",
    "# configurar de acuerdo a dónde bajaron los sets de imágenes\n",
    "TRAIN_DIR = Path('./ucse-ia-2024-tp-2-clasificacion-de-imagenes/train')\n",
    "TEST_DIR = Path('./ucse-ia-2024-tp-2-clasificacion-de-imagenes/test')\n",
    "\n",
    "SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb713d-2237-4cea-b8d9-a12f6329bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_reader = ImageDataGenerator(\n",
    "    rescale=1/255,  # Normaliza los píxeles a valores entre 0 y 1\n",
    "    rotation_range=20,  # Rotar imágenes aleatoriamente entre 0 y 20 grados\n",
    "    width_shift_range=0.2,  # Desplazar horizontalmente hasta el 20% del ancho\n",
    "    height_shift_range=0.2,  # Desplazar verticalmente hasta el 20% de la altura\n",
    "    shear_range=0.2,  # Cortar (shear) imágenes aleatoriamente\n",
    "    zoom_range=0.2,  # Ampliar imágenes aleatoriamente\n",
    "    horizontal_flip=True,  # Voltear imágenes horizontalmente\n",
    "    fill_mode='nearest',  # Estrategia para rellenar nuevos píxeles\n",
    "    brightness_range=(0.5, 1.5),  # Cambiar brillo aleatoriamente\n",
    "    validation_split=0.4  # Porcentaje para la validación (40%)\n",
    ")\n",
    "\n",
    "READ_PARAMS = dict(\n",
    "    class_mode=\"categorical\",\n",
    "    classes=LANDSCAPE_TYPE, # para usar el mismo orden en todos lados\n",
    "    target_size=(SIZE, SIZE), # para que corra más rápido, vamos a achicar las imágenes\n",
    "    color_mode=\"rgb\",  # queremos trabajar con las imágenes a color\n",
    "    batch_size=32, # Tamaño del lote\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921678d-1b7d-4a26-8342-6889181e97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generador para entrenamiento (60%)\n",
    "train = images_reader.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    subset='training',\n",
    "    **READ_PARAMS\n",
    ")\n",
    "\n",
    "# Generador para validación (40%)\n",
    "validation = images_reader.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    subset='validation',\n",
    "    **READ_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e630321-22ae-4a94-9612-09024c5cffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images_train(dataset):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    images, labels = next(dataset)\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(LANDSCAPE_TYPE[np.argmax(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821af4e-3ac9-42d3-a7f2-f8355b3b0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images_train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa203c-b6c3-4252-bc59-ddebf3a762ac",
   "metadata": {},
   "source": [
    "# 1 Análisis exploratorio sobre el conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49242402-193c-43b8-b3a7-15f251a6a9f3",
   "metadata": {},
   "source": [
    "## 1.1 Volumetría de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90dfc6-f197-49a7-8916-c45c9586a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_by_category(dataset_dir):\n",
    "    categories = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "    category_counts = {}\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(dataset_dir, category)\n",
    "        image_files = [f for f in os.listdir(category_path) if f.endswith(('jpg', 'jpeg', 'png'))]\n",
    "        category_counts[category] = len(image_files)\n",
    "\n",
    "    total_images = sum(category_counts.values())\n",
    "\n",
    "    return category_counts, total_images\n",
    "\n",
    "# Obtener la volumetría de los datos\n",
    "category_counts, total_images = count_images_by_category(TRAIN_DIR)\n",
    "\n",
    "print(\"Volumetría del dataset:\")\n",
    "print(f\"Total de imágenes: {total_images}\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count} imágenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef900-4177-46a6-b074-ed7b36ecfa06",
   "metadata": {},
   "source": [
    "## 1.2 Estructura y tipo de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da7819-b91f-4252-9f3e-b62fc2a80831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_dimensions(dataset_dir):\n",
    "    image_dimensions = []\n",
    "    image_formats = []\n",
    "    for root, dirs, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = Image.open(img_path)\n",
    "                image_dimensions.append(img.size)  # Obtiene el tamaño (ancho, alto)\n",
    "                image_formats.append(img.format)  # Obtiene el formato de la imagen\n",
    "    \n",
    "    return image_dimensions, image_formats\n",
    "\n",
    "# Obtener la estructura de las imágenes\n",
    "image_dimensions, image_formats = get_image_dimensions(TRAIN_DIR)\n",
    "\n",
    "# Muestra algunos ejemplos de dimensiones y formatos\n",
    "print(\"Ejemplos de dimensiones de imágenes:\")\n",
    "for i in range(5):\n",
    "    print(f\"Imagen {i+1}: {image_dimensions[i]} (Formato: {image_formats[i]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c63e0-98a2-4fea-8f66-200f35a210be",
   "metadata": {},
   "source": [
    "## 1.3 Distribución de la variable a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968aedb6-45c7-4b73-af1a-18aa80783f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_distribution(category_counts):\n",
    "    categories = list(category_counts.keys())\n",
    "    counts = list(category_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(categories, counts, color='skyblue')\n",
    "    plt.xlabel('Categorías')\n",
    "    plt.ylabel('Número de imágenes')\n",
    "    plt.title('Distribución de imágenes por categoría')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Graficar la distribución de las imágenes por categoría\n",
    "plot_image_distribution(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88a537-2866-4ea2-b5c6-09b639adaab6",
   "metadata": {},
   "source": [
    "# 2 Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c296a-1aa4-4c5c-8bae-340e74bceb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "\n",
    "model_weights_at_epochs = {}\n",
    "\n",
    "class OurCustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model_weights_at_epochs[epoch] = self.model.get_weights()\n",
    "\n",
    "# Función para plotear el historial de entrenamiento\n",
    "def PlotHistory(history):\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='validation')\n",
    "    plt.title('Accuracy over train epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "class MLPBuilder:\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_shape=(SIZE, SIZE, 3), \n",
    "        output_size=len(LANDSCAPE_TYPE),\n",
    "    ):\n",
    "        self.input_shape = input_shape  # Ej: (SIZE, SIZE, 3)\n",
    "        self.output_size = output_size  # Ej: len(LANDSCAPE_TYPE)\n",
    "        self.model = None\n",
    "    \n",
    "    def build_model(self, layers_config, batch_normalization=False):\n",
    "        \"\"\"\n",
    "        Construye un modelo secuencial MLP basado en la configuración proporcionada.\n",
    "\n",
    "        :param layers_config: Lista de diccionarios con la configuración de cada capa. \n",
    "                              Ej: [{'units': 1024, 'activation': 'tanh', 'dropout': 0.25}, {...}]\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # Primera capa: Flatten (Aplanar las imágenes)\n",
    "        self.model.add(Flatten(input_shape=self.input_shape))\n",
    "        \n",
    "        # Agregar capas ocultas configuradas\n",
    "        for layer in layers_config:            \n",
    "            if layer['activation'] == 'leakyrelu':\n",
    "                self.model.add(Dense(layer['units']))\n",
    "                self.model.add(LeakyReLU(alpha=0.01))\n",
    "            else:\n",
    "                self.model.add(Dense(layer['units'], activation=layer['activation']))\n",
    "\n",
    "            if batch_normalization:\n",
    "                self.model.add(BatchNormalization())\n",
    "\n",
    "            if 'dropout' in layer:\n",
    "                self.model.add(Dropout(layer['dropout']))\n",
    "        \n",
    "        # Capa de salida (softmax para clasificación multiclase)\n",
    "        self.model.add(Dense(self.output_size, activation='softmax'))\n",
    "\n",
    "    def compile_model(self, optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']):\n",
    "        \"\"\"\n",
    "        Compila el modelo con el optimizador, la función de pérdida y las métricas indicadas.\n",
    "        \"\"\"\n",
    "        if self.model:\n",
    "            self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "            self.model.summary()\n",
    "        else:\n",
    "            raise ValueError(\"El modelo no está construido. Llama a build_model primero.\")\n",
    "    \n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        Devuelve el modelo generado.\n",
    "        \"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def fit_model(self, train=train, validation=validation, epochs=10, batch_size=32, callbacks=[OurCustomCallback()]):\n",
    "        \"\"\"\n",
    "        Entrena el modelo con 10 épocas por default.\n",
    "        \"\"\"\n",
    "        return PlotHistory(self.model.fit(\n",
    "            train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=validation,\n",
    "            callbacks=callbacks,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a03803-875a-4c42-b4f9-62a5bf76249e",
   "metadata": {},
   "source": [
    "## Modelo MLP 1024 - 512 - 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755e785-76b4-4db0-90f1-97444d7f5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de capas del MLP\n",
    "layers_config = [\n",
    "    {'units': 1024, 'activation': 'relu', 'dropout': 0.25},\n",
    "    {'units': 512, 'activation': 'relu', 'dropout': 0.25},\n",
    "    {'units': 256, 'activation': 'relu', 'dropout': 0.25}\n",
    "]\n",
    "\n",
    "mlp_model_builder = MLPBuilder()\n",
    "mlp_model_builder.build_model(layers_config)\n",
    "mlp_model_builder.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76151c-d430-41d6-bdf3-e2293150e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrena el modelo y lo plotea\n",
    "mlp_model_builder.fit_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769dc9b-3ed5-4f37-9f3e-4f23dea4bd34",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Este primer modelo MLP que probamos no está alcanzando una buena precisión, tanto en el conjunto de entrenamiento como en el de validación. La precisión se mantiene baja (~0.26) y la pérdida es relativamente alta. \n",
    "\n",
    "El número de neuronas (1024, 512, 256) podría ser demasiado grande para el problema. Vamos a reducirlas para evitar el sobreajuste, dado que el modelo parece estar teniendo dificultades para generalizar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c887dc-24d5-4b23-81c5-e45f5ebec826",
   "metadata": {},
   "source": [
    "## Modelo MLP 256 - 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49716482-4ac7-4145-842c-fb28a6266ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de capas del MLP\n",
    "layers_config = [\n",
    "    {'units': 256, 'activation': 'relu', 'dropout': 0.25},\n",
    "    {'units': 128, 'activation': 'relu', 'dropout': 0.25},\n",
    "]\n",
    "\n",
    "mlp_model_builder = MLPBuilder()\n",
    "mlp_model_builder.build_model(layers_config)\n",
    "mlp_model_builder.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943952d6-92bf-4b2d-bfdf-6702ea65f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrena el modelo y lo plotea\n",
    "mlp_model_builder.fit_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1432b-e6c7-49d4-adcc-2154cf87b00f",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "El nuevo modelo simplificado muestra una mejora en la precisión, aunque sigue habiendo espacio para optimizarlo. Actualmente, la precisión en el conjunto de validación se estabiliza entre un 30% y 35%, lo que indica que el modelo aún no está capturando de manera efectiva todas las características del conjunto de datos. Se puede observar que en la época 8 el modelo alcanza su límite de rendimiento, comenzando a presentar signos de sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940d8a5-0b39-43c4-b7ce-dde26c20c702",
   "metadata": {},
   "source": [
    "## Modelo MLP con LeakyReLU y Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59014b-1e60-48c3-9fc4-4e7bd78b2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_config = [\n",
    "    {'units': 256, 'activation': 'leakyrelu', 'dropout': 0.15},\n",
    "    {'units': 128, 'activation': 'leakyrelu', 'dropout': 0.15},\n",
    "]\n",
    "\n",
    "mlp_model_builder = MLPBuilder()\n",
    "mlp_model_builder.build_model(layers_config, batch_normalization=True)\n",
    "mlp_model_builder.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34d761-51cd-4177-8988-ab824fdafc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrena el modelo y lo plotea\n",
    "mlp_model_builder.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7834b5-763e-4c09-a516-327ddfbc1b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
